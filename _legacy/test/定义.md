```
Agent
Environment

action
observation, reward

env.dealwith(action)
env.update(observation)
env.update(reward)
agent.learn(action,observation,reward)
```



机器人：组件、器官、功能的整体
组件：提供身体结构和动作
器官：提供信号输入或特殊功能

器官-大脑：形成感受、形成目标
器官-电池：提供能源

动作：就是动作啦
目标：根据感受、状况，决定要做什么
代理：对于每一个行动目标，建立一个DQ网络，进行动作输出




robot

goal

agent

brain

battery

component

organ




**触**
观察口
观察口.值
观察口.索引
观察口.名字
观察口.描述
观察口.某状态下的标准值(状态)


次级观察口


感受观察口


达成加分，不达成无效
达成扣分，不达成无效
达成加分，不达成扣分


状况
状况.观察口与其标准值列表
状况.索引
状况.名字
状况.描述

**受**

舒服，加分
不爽，扣分
无感，不加不扣


```
function 感受(观察口列表) {
  this.得分 = 计算(观察口列表[i].值);
  this.苦受范围 = [-10000,-300];
  this.乐受范围 = [300,10000];
  this.感受类型 = 苦受;//苦受乐受舍受
  this.权重 = -1;//苦受-1，乐受1，舍受0
}

function 计算受(观察口的值x) {
  var x = 观察口的值x;
  var so = new Array();
  var kusoline = -424;
  var lesoline = 435;
  if (x<kusoline) {
    sotype = -1;
    sovalue = kusoline-x;
  }
  else if (x>lesoline) {
    sotype = 1;
    sovalue = x-lesoline;
  }
  else {
    sotype = 0;
    sovalue = 0;
  }
  so.type = sotype;
  so.value = sovalue;
  return so;
}
```

感受
感受.值
感受.观察口列表
感受.计算方法
感受.索引
感受.名字
感受.描述

活动：一系列的动作，为了达到一定的状况
活动.目标状况
活动.过程状况列表
活动.索引
活动.名字
活动.描述


规划：从当前状况到目标状况




作意
观察口权重
观察口期望值


制定造作

造作




状态

根据身体感受计算得分
对状态进行奖励

根据心理感受计算得分
对目标进行奖励

根据目标计算得分
对动作规划进行奖励

根据动作规划计算得分
对动作进行奖励

动作（前进，退后，旋转）
对状态进行改变


目标
goal(robot,target)



定义感受
提供观察口
需要大脑

定义动作
消耗能量
需要电池


作意
触
受
想
思

欲
胜解
念
定
慧









-------------------------------------------------------------------------

-------------------------------------------------------------------------

-------------------------------------------------------------------------

-------------------------------------------------------------------------


For most applications (e.g. simple games), the DQN algorithm is a safe bet to use. If your project has a finite state space that is not too large, the DP or tabular TD methods are more appropriate. As an example, the DQN Agent satisfies a very simple API:

```
// create an environment object
var env = {};
env.getNumStates = function() { return 8; }
env.getMaxNumActions = function() { return 4; }

// create the DQN agent
var spec = { alpha: 0.01 } // see full options on DQN page
agent = new RL.DQNAgent(env, spec); 

setInterval(function(){ // start the learning loop
  var action = agent.act(s); // s is an array of length 8
  //... execute action in environment and get the reward
  agent.learn(reward); // the agent improves its Q,policy,model, etc. reward is a float
}, 0);

```

In other words, you pass the agent some vector and it gives you an action. Then you reward or punish its behavior with the `reward` signal. The agent will over time tune its parameters to maximize the rewards it obtains.





As for the available parameters in the DQN agent `spec`:

- `spec.gamma` is the discount rate. When it is zero, the agent will be maximally greedy and won't plan ahead at all. It will grab all the reward it can get right away. For example, children that fail the marshmallow experiment have a very low gamma. This parameter goes up to 1, but cannot be greater than or equal to 1 (this would make the discounted reward infinite).
- `spec.epsilon` controls the epsilon-greedy policy. High epsilon (up to 1) will cause the agent to take more random actions. It is a good idea to start with a high epsilon (e.g. 0.2 or even a bit higher) and decay it over time to be lower (e.g. 0.05).
- `spec.num_hidden_units`: currently the DQN agent is hardcoded to use a neural net with one hidden layer, the size of which is controlled with this parameter. For each problems you may get away with smaller networks.
- `spec.alpha` controls the learning rate. Everyone sets this by trial and error and that's pretty much the best thing we have.
- `spec.experience_add_every`: REINFORCEjs won't add a new experience to replay every single frame to try to conserve resources and get more variaty. You can turn this off by setting this parameter to 1. Default = 5
- `spec.experience_size`: size of memory. More difficult problems may need bigger memory
- `spec.learning_steps_per_iteration`: the more the better, but slower. Default = 20
- `spec.tderror_clamp`: for robustness, clamp the TD Errror gradient at this value.



- `spec.gamma`：折扣几率。0时最贪心，不会长远计划。
- `spec.epsilon`：控制贪婪策略。越接近一，行为越随机。
- `spec.num_hidden_units`：目前只有一层神经网络，其容量由此控制。
- `spec.alpha` ：控制学习几率。Everyone sets this by trial and error and that's pretty much the best thing we have.
- `spec.experience_add_every`：每几次学习一次。
- `spec.experience_size`：记忆容量。
- `spec.learning_steps_per_iteration`：每次重复的学习步数。
- `spec.tderror_clamp`：为了鲁棒性，在这个值夹住 TD Errror 的斜率。